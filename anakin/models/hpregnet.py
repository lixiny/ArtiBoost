import os
import pickle
from collections import OrderedDict
from typing import Dict

import torch
import torch.nn as nn
from manotorch.utils.rodrigues import rodrigues
from anakin.datasets.hoquery import Queries
from anakin.models.mano import ManoAdaptor
from anakin.utils.builder import MODEL, build_backbone, build_head
from anakin.utils.transform import batch_persp_proj2d, compute_rotation_matrix_from_ortho6d
from anakin.utils.logger import logger
from anakin.utils.misc import CONST, enable_lower_param
from anakin.utils.netutils import recurse_freeze


@MODEL.register_module
class HOPRegNet(nn.Module):

    @enable_lower_param
    def __init__(self, **cfg):
        super(HOPRegNet, self).__init__()

        if cfg["BACKBONE"]["PRETRAINED"] and cfg["PRETRAINED"]:
            logger.warning(f"{type(self).__name__}'s backbone {cfg['BACKBONE']['TYPE']} "
                           f"weights will be rewritten by {cfg['PRETRAINED']}")

        self.inp_res = cfg["DATA_PRESET"]["IMAGE_SIZE"]
        self.feature_dim = cfg["HEAD"]["INPUT_DIM"]
        self.center_idx = cfg["DATA_PRESET"]["CENTER_IDX"]
        logger.info(f"{type(self).__name__} uses center_idx {self.center_idx}")

        self.base_net = build_backbone(cfg["BACKBONE"])  # ResNet18
        self.mano_branch = build_head(cfg["HEAD"], default_args=cfg["DATA_PRESET"])

        self.obj_transfhead = HOPRegNet.TransHead(inp_dim=self.feature_dim, out_dim=9)
        self.proj2d_func = batch_persp_proj2d

        if cfg.get("MANO_FHB_ADAPTOR", False):
            mano_fhb_adaptor_dir = cfg["MANO_FHB_ADAPTOR_DIR"]  # assets/hasson20_assets/mano
            adaptor_path = os.path.join(mano_fhb_adaptor_dir, f"fhb_skel_centeridx{self.center_idx}.pkl")
            with open(adaptor_path, "rb") as p_f:
                exp_data = pickle.load(p_f)
            self.register_buffer("fhb_shape", torch.Tensor(exp_data["shape"]))
            self.adaptor = ManoAdaptor(self.mano_branch.mano_layer, adaptor_path)
            recurse_freeze(self.adaptor)
        else:
            self.adaptor = None

        self.init_weights(pretrained=cfg["PRETRAINED"])

    class TransHead(nn.Module):

        def __init__(self, inp_dim: int, out_dim: int):
            super().__init__()

            if out_dim != 3 and out_dim != 9:
                logger.error(f"Unrecognized TransHead out dim: {out_dim}")
                raise ValueError()

            base_neurons = [inp_dim, int(inp_dim / 2)]
            layers = []
            for (inp_neurons, out_neurons) in zip(base_neurons[:-1], base_neurons[1:]):
                layers.append(nn.Linear(inp_neurons, out_neurons))
                layers.append(nn.ReLU())
            self.final_layer = nn.Linear(out_neurons, out_dim)
            self.decoder = nn.Sequential(*layers)

        def forward(self, inp):
            decoded = self.decoder(inp)
            out = self.final_layer(decoded)
            return out

    def recover_mano(self, feature: torch.Tensor, samples: Dict):
        # ============= Get hand joints & verts, centered >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
        mano_results = self.mano_branch(feature)

        if self.adaptor:
            """
            HASSON[CVPR2020] MANO Adaptor for FHAB dataset
            """
            verts = mano_results["hand_verts_3d"]
            adapt_joints, _ = self.adaptor(verts)
            adapt_joints = adapt_joints.transpose(1, 2)
            mano_results["joints_3d"] = adapt_joints - adapt_joints[:, self.center_idx].unsqueeze(1)
            mano_results["hand_verts_3d"] = verts - adapt_joints[:, self.center_idx].unsqueeze(1)
        # ============== Recover hand position in camera coordinates >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

        cam_intr = samples[Queries.CAM_INTR]  # TENSOR(B, 3, 3)
        root_joint = samples[Queries.ROOT_JOINT]  # [B, 3]
        joints_3d_abs = mano_results["joints_3d"] + root_joint.unsqueeze(1)
        hand_verts_3d_abs = mano_results["hand_verts_3d"] + root_joint.unsqueeze(1)

        joints_2d = self.proj2d_func(joints_3d_abs, cam_intr)
        hand_verts_2d = self.proj2d_func(hand_verts_3d_abs, cam_intr)

        mano_results["joints_2d"] = joints_2d
        mano_results["root_joint"] = root_joint
        mano_results["joints_3d_abs"] = joints_3d_abs
        mano_results["hand_verts_3d_abs"] = hand_verts_3d_abs
        mano_results["hand_verts_2d"] = hand_verts_2d

        return mano_results

    def recover_object(self, feature: torch.Tensor, samples: Dict):
        """
        Compute object vertex and corner positions in camera coordinates by predicting object translation
        and scaling, and recovering 3D positions given known object model
        """
        transf_obj = self.obj_transfhead(feature)
        batch_size = transf_obj.shape[0]
        tsl_wrt_hand = transf_obj[:, :3]  # [B, 3]
        box_rot_6d = transf_obj[:, 3:]

        rotmat = compute_rotation_matrix_from_ortho6d(box_rot_6d).view(batch_size, 3, 3)

        root_joint = samples[Queries.ROOT_JOINT]  # [B, 3]

        cam_intr = samples[Queries.CAM_INTR]
        obj_center = root_joint + tsl_wrt_hand

        corners_can = samples[Queries.CORNERS_CAN]
        obj_corners_ = rotmat.bmm(corners_can.float().transpose(1, 2)).transpose(1, 2)
        corners_3d_abs = obj_corners_ + obj_center.unsqueeze(1)
        corners_2d = self.proj2d_func(corners_3d_abs, cam_intr)

        obj_results = {
            "obj_center": obj_center,
            "corners_3d_abs": corners_3d_abs,
            "obj_pred_tsl": tsl_wrt_hand,
            "obj_pred_rot": rotmat,
            "corners_2d": corners_2d,
            # TODO for MSSD
            "box_rot_rotmat": rotmat,
            "boxroot_3d_abs": obj_center,
        }

        return obj_results

    def forward(self, samples: Dict):
        results = {}
        features = self.base_net(image=samples["image"])
        mano_results = self.recover_mano(features["res_layer4_mean"], samples)
        obj_results = self.recover_object(features["res_layer4_mean"], samples)

        # â¬‡ make corners_3d root relative
        obj_results["corners_3d"] = obj_results["corners_3d_abs"] - mano_results["root_joint"].unsqueeze(1)
        results = {**mano_results, **obj_results}
        return results

    def init_weights(self, pretrained=""):
        if pretrained == "":
            logger.warning(f"=> Init {type(self).__name__} weights in backbone and head")
            """
            Add init for other modules if has
            ...
            """
        elif os.path.isfile(pretrained):
            # pretrained_state_dict = torch.load(pretrained)
            logger.info(f"=> Loading {type(self).__name__} pretrained model from: {pretrained}")
            # self.load_state_dict(pretrained_state_dict, strict=False)
            checkpoint = torch.load(pretrained)
            if isinstance(checkpoint, OrderedDict):
                state_dict = checkpoint
            elif isinstance(checkpoint, dict) and "state_dict" in checkpoint:
                state_dict_old = checkpoint["state_dict"]
                state_dict = OrderedDict()
                # delete 'module.' because it is saved from DataParallel module
                for key in state_dict_old.keys():
                    if key.startswith("module."):
                        # state_dict[key[7:]] = state_dict[key]
                        # state_dict.pop(key)
                        state_dict[key[7:]] = state_dict_old[key]  # delete "module." (in nn.parallel and ddp)
                    else:
                        state_dict[key] = state_dict_old[key]
            else:
                logger.error(f"=> No state_dict found in checkpoint file {pretrained}")
                raise RuntimeError()
            self.load_state_dict(state_dict, strict=True)
        else:
            logger.error(f"=> No {type(self).__name__} checkpoints file found in {pretrained}")
            raise FileNotFoundError()
